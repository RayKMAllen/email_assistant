### Technical Explanation

#### Reasoning for Chosen AWS Services and LLM Models

- **AWS Bedrock** was chosen primarily because it provides access to multiple AWS services via a single API, eliminating the need for further time-consuming setup and configuration. While the project does not currently rely on other AWS services (such as S3, DynamoDB, or Lambda), the broader AWS ecosystem was a strong consideration.
- **Anthropic Claude 3.7 Sonnet** was chosen for its strong performance on human-style communication. A recent blind comparison of LLMs (https://www.washingtonpost.com/technology/2025/03/26/best-ai-email-assistant/) found that Claude slightly outperformed competitors in this area. I chose Sonnet 3.7 rather than the newer Sonnet 4 since the latter's superior performance on coding and mathematics (https://www.edenai.co/post/claude-sonnet-3-7-vs-claude-sonnet-4) is not useful here, and its performance on generalized reasoning is no better.
In practice any of the major LLMs would likely be hard to distinguish for this purpose.

#### Chatbot Context Maintenance

- The key information when iterating draft replies is a summary of the original conversation, and the previous draft. These are stored in the `BedrockSession` class object and used for refinement.
 - The `BedrockSession` class additionally maintains a `history` list, which stores the sequence of user prompts and model responses.

#### Email Processing Implementation

- Email content is loaded via `process_path_or_email`, which supports both file paths (including PDF extraction) and raw text.
- Key information extraction and summarization are performed by sending structured prompts to the LLM, which returns JSON-formatted results.
- Draft replies are generated by composing prompts with optional tone instructions and passing them to the LLM.
- Refinement is supported by sending the last draft, user instructions and summary back to the LLM for iterative improvement.

#### Data Flow Between Components

1. **CLI (`cli/cli.py`)**: Handles user input and command parsing.
2. **Session (`assistant/llm_session.py`)**: Manages LLM interaction, context, and state.
3. **Utils (`assistant/utils.py`)**: Handles file reading, PDF extraction, and draft saving.
4. **Flow**: User input → CLI → Session → (optionally) Utils (text extraction) → Session -> LLM (via Bedrock API) → Session → (optionally) Utils (saving to disk) → Session → CLI → Output to user.

#### Error Handling and Fallback Strategies

- **JSON Parsing**: When extracting key info, the code attempts to parse the LLM output as JSON. If parsing fails, an error message is printed and the operation is safely aborted.
- **Missing Information**: If the user tries to draft or refine a reply before the email conversation is successfully loaded, the system prints a warning and aborts. Similarly, if a user tries to refine or save a draft before one exists, the system prints a warning and either drafts a reply automatically or aborts the save.
- **General Robustness**: All CLI commands are wrapped with checks and try/except blocks to prevent crashes and display helpful information to the user in case of errors.