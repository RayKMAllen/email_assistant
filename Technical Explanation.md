### Technical Explanation

#### Reasoning for Chosen AWS Services and LLM Models

- **AWS Bedrock** was chosen primarily because it provides access to multiple AWS services via a single API, eliminating the need for further time-consuming setup and configuration. While the project does not currently rely on other AWS services, the availability of the broader AWS ecosystem if necessary was a consideration.
- **Anthropic Claude 3.7 Sonnet** was chosen for its strong performance on human-style communication. A recent blind comparison of LLMs (https://www.washingtonpost.com/technology/2025/03/26/best-ai-email-assistant/) found that Claude slightly outperformed competitors in this area. I chose Sonnet 3.7 rather than the newer Sonnet 4 since the latter's superior performance on coding and mathematics (https://www.edenai.co/post/claude-sonnet-3-7-vs-claude-sonnet-4) is not useful here, and its performance on generalized reasoning is no better.
In practice any of the major LLMs would likely be hard to distinguish for simple, professional emails.

#### Chatbot Context Maintenance

- The key information when iterating draft replies is a summary of the original email conversation, and the previous draft. These are stored in the `BedrockSession` class object. By default, this information is formatted together with any new instructions to form the prompt for refining.
 - The `BedrockSession` class additionally maintains a `history` list, which stores the sequence of user prompts and model responses. The user has the option to include this instead of the summary in the refining prompt. This uses many more tokens and is likely only to be useful if the user wishes to reference a specific earlier draft.

#### Email Processing Implementation

- Email content is loaded via `process_path_or_email`, which supports both file paths (including PDF extraction) and raw text.
- Key information extraction and summarization are performed by sending structured prompts to the LLM, which returns JSON-formatted results.
- Draft replies are generated by composing prompts with optional tone instructions and passing them to the LLM.
- Refinement is supported by sending the last draft, user instructions and summary back to the LLM for iterative improvement.

#### Data Flow Between Components

1. **CLI (`cli/cli.py`)**: Handles user input and command parsing.
2. **Session (`assistant/llm_session.py`)**: Manages LLM interaction, context, and state.
3. **Utils (`assistant/utils.py`)**: Handles file reading, PDF extraction, and draft saving.
4. **Flow**: User input → CLI → Session → (optionally) Utils (text extraction) → Session -> LLM (via Bedrock API) → Session → (optionally) Utils (saving to disk) → Session → CLI → Output to user.

#### Error Handling and Fallback Strategies

- **JSON Parsing**: If LLM output can't be parsed as JSON, an error is shown and the step is skipped.
- **Boto3 Errors**: AWS errors (e.g., `ClientError`) are caught to prevent crashes and show helpful messages.
- **Missing Information**: If the user tries to draft or refine a reply before the email conversation is successfully loaded, the system prints a warning and aborts. Similarly, if a user tries to refine or save a draft before one exists, the system prints a warning and either drafts a reply automatically or aborts the save.
- **General Robustness**: All CLI commands are wrapped with checks and try/except blocks to prevent crashes and display helpful information to the user in case of errors.

#### Development and Testing Environment

- **Operating System**: Windows 11 Pro
- **Python Version**: 3.12.4
- **IDE**: Visual Studio Code with Python and AWS extensions